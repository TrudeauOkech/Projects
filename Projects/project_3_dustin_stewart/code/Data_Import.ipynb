{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fresh-closing",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "We are pulling data from the pushshift API. I'll be pulling 5000 rows of post data from the [r/bipolar](https://www.reddit.com/r/bipolar/) and [r/depression](https://www.reddit.com/r/depression/) subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-massage",
   "metadata": {},
   "source": [
    "# \n",
    "These are the imports we will use. Requests will allow us to call on the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "saved-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-south",
   "metadata": {},
   "source": [
    "# \n",
    "### Create a Function to Pull Data\n",
    "This is a function I created that takes in a subreddit name and an amount of rows and creates a dataframe with that many rows of data from that subreddit. It saves a global variable in the subreddit name and also saves the dataframe as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precise-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_df_creator(subreddit, rows):\n",
    "        \n",
    "        \n",
    "        # establish base url for pulling posts\n",
    "        base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "        \n",
    "        # establish a count so our iterations don't bring back the same data over and over\n",
    "        count = 0\n",
    "        \n",
    "        # create a list that we will append with the reddit posts that meet our conditions\n",
    "        json_data = []\n",
    "        \n",
    "        # create a while loop that will continue until we have the necessary amount of data\n",
    "        while len(json_data) < (rows + rows*(.01)):\n",
    "            \n",
    "            # create parameters for our api requests so they will be less intensive\n",
    "            new_params = {'subreddit': subreddit, \n",
    "                'fields': ['selftext', 'subreddit', 'title'], \n",
    "                \n",
    "                # these parameters refer to the timeframe to pull posts from, \n",
    "                # which using our counter, goes back one day further after each iteration         \n",
    "                'after': (str(count + 1) + 'd'), \n",
    "                'before': (str(count) + 'd')}\n",
    "            \n",
    "            # make the request to the api\n",
    "            request = requests.get(base_url, new_params)\n",
    "            \n",
    "            # convert the requested data to JSON\n",
    "            try:\n",
    "                json = request.json()\n",
    "            except:\n",
    "                count = count + 1\n",
    "                continue\n",
    "            \n",
    "            # get the nested json within the data key\n",
    "            json_data_only = json['data']\n",
    "            \n",
    "            # filter out posts with less than 40 characters in their post\n",
    "            for i in json_data_only:\n",
    "                try:\n",
    "                    if len(i['selftext'].split()) > 100:\n",
    "                        json_data.append(i)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # increase the counter by 1 so we call on posts from the previous day in the next iteration\n",
    "            count = count + 1\n",
    "            \n",
    "            \n",
    "            # printing number of rows added so far to track progress\n",
    "            print('Subreddit posts added to dataframe: ' + str(len(json_data)))\n",
    "        \n",
    "        \n",
    "        # turn list of json dictionaries into a pandas dataframe using json_normalize, save as global variable,\n",
    "        # drop duplicates, and slice datframe to be desired size\n",
    "        globals()[subreddit] = pd.json_normalize(json_data)\n",
    "        globals()[subreddit] = globals()[subreddit].drop_duplicates()\n",
    "        globals()[subreddit] = globals()[subreddit].iloc[:rows]\n",
    "        \n",
    "        # save dataframe as a csv file\n",
    "        #globals()[subreddit].to_csv('../data/' + subreddit + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-arrest",
   "metadata": {},
   "source": [
    "# \n",
    "This is a demonstration of the output, limited to 100 posts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "other-accountability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit posts added to dataframe: 0\n",
      "Subreddit posts added to dataframe: 0\n",
      "Subreddit posts added to dataframe: 14\n",
      "Subreddit posts added to dataframe: 26\n",
      "Subreddit posts added to dataframe: 36\n",
      "Subreddit posts added to dataframe: 48\n",
      "Subreddit posts added to dataframe: 58\n",
      "Subreddit posts added to dataframe: 67\n",
      "Subreddit posts added to dataframe: 77\n",
      "Subreddit posts added to dataframe: 89\n",
      "Subreddit posts added to dataframe: 97\n",
      "Subreddit posts added to dataframe: 110\n",
      "Subreddit posts added to dataframe: 11\n",
      "Subreddit posts added to dataframe: 21\n",
      "Subreddit posts added to dataframe: 28\n",
      "Subreddit posts added to dataframe: 37\n",
      "Subreddit posts added to dataframe: 48\n",
      "Subreddit posts added to dataframe: 55\n",
      "Subreddit posts added to dataframe: 61\n",
      "Subreddit posts added to dataframe: 71\n",
      "Subreddit posts added to dataframe: 77\n",
      "Subreddit posts added to dataframe: 84\n",
      "Subreddit posts added to dataframe: 90\n",
      "Subreddit posts added to dataframe: 96\n",
      "Subreddit posts added to dataframe: 101\n"
     ]
    }
   ],
   "source": [
    "subreddit_df_creator('depression', 100)\n",
    "subreddit_df_creator('bipolar', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-chassis",
   "metadata": {},
   "source": [
    "# \n",
    "**Now that we have our data, we can take a look at it:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-music",
   "metadata": {},
   "source": [
    "# \n",
    "**Up Next:**  \n",
    "[Cleaning and Exploratory Analysis](./Cleaning_and_Exploratory_Analysis.ipynb)  \n",
    "  \n",
    "[Return to Read Me](../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-liechtenstein",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

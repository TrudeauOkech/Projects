{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blocked-clerk",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-caution",
   "metadata": {},
   "source": [
    "# \n",
    "These are our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "preliminary-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-howard",
   "metadata": {},
   "source": [
    "# \n",
    "Bring in the cleaned data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "labeled-income",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sentences</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am now homeless and my phone service will tu...</td>\n",
       "      <td>0</td>\n",
       "      <td>['homeless', 'phone', 'service', 'turn', 'real...</td>\n",
       "      <td>i am now homeless and my phone service will tu...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People always describe their depression as con...</td>\n",
       "      <td>0</td>\n",
       "      <td>['people', 'always', 'describe', 'depression',...</td>\n",
       "      <td>people always describe their depression a cons...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been struggling really hard with this. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['struggling', 'really', 'hard', 'graduated', ...</td>\n",
       "      <td>i have been struggling really hard with this w...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So Its been 1 year I had my future secure and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['1', 'year', 'future', 'secure', 'well', 'wa'...</td>\n",
       "      <td>so it been 1 year i had my future secure and w...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am 15 yrs old and I just need some help.\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>['15', 'yr', 'old', 'need', 'help', 'thing', '...</td>\n",
       "      <td>i am 15 yr old and i just need some help thing...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I honestly love the flavour of them!\\n\\nI star...</td>\n",
       "      <td>1</td>\n",
       "      <td>['honestly', 'love', 'flavour', 'started', 're...</td>\n",
       "      <td>i honestly love the flavour of them i started ...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Three days on the trot I have been unable to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['three', 'day', 'trot', 'unable', 'sleep', 'e...</td>\n",
       "      <td>three day on the trot i have been unable to sl...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Just wanted to say that I have been feeling go...</td>\n",
       "      <td>1</td>\n",
       "      <td>['wanted', 'say', 'feeling', 'good', 'last', '...</td>\n",
       "      <td>just wanted to say that i have been feeling go...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Hey guys. I just wanted to ask you if you have...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...</td>\n",
       "      <td>hey guy i just wanted to ask you if you have e...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Hey, all!\\n\\nI posted a few days ago about how...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'posted', 'day', 'ago', 'wa', 'going',...</td>\n",
       "      <td>hey all i posted a few day ago about how i wa ...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               selftext  subreddit  \\\n",
       "0     I am now homeless and my phone service will tu...          0   \n",
       "1     People always describe their depression as con...          0   \n",
       "2     I have been struggling really hard with this. ...          0   \n",
       "3     So Its been 1 year I had my future secure and ...          0   \n",
       "4     I am 15 yrs old and I just need some help.\\n\\n...          0   \n",
       "...                                                 ...        ...   \n",
       "9995  I honestly love the flavour of them!\\n\\nI star...          1   \n",
       "9996  Three days on the trot I have been unable to s...          1   \n",
       "9997  Just wanted to say that I have been feeling go...          1   \n",
       "9998  Hey guys. I just wanted to ask you if you have...          1   \n",
       "9999  Hey, all!\\n\\nI posted a few days ago about how...          1   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     ['homeless', 'phone', 'service', 'turn', 'real...   \n",
       "1     ['people', 'always', 'describe', 'depression',...   \n",
       "2     ['struggling', 'really', 'hard', 'graduated', ...   \n",
       "3     ['1', 'year', 'future', 'secure', 'well', 'wa'...   \n",
       "4     ['15', 'yr', 'old', 'need', 'help', 'thing', '...   \n",
       "...                                                 ...   \n",
       "9995  ['honestly', 'love', 'flavour', 'started', 're...   \n",
       "9996  ['three', 'day', 'trot', 'unable', 'sleep', 'e...   \n",
       "9997  ['wanted', 'say', 'feeling', 'good', 'last', '...   \n",
       "9998  ['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...   \n",
       "9999  ['hey', 'posted', 'day', 'ago', 'wa', 'going',...   \n",
       "\n",
       "                                              sentences  word_count  \n",
       "0     i am now homeless and my phone service will tu...         234  \n",
       "1     people always describe their depression a cons...         155  \n",
       "2     i have been struggling really hard with this w...         394  \n",
       "3     so it been 1 year i had my future secure and w...         105  \n",
       "4     i am 15 yr old and i just need some help thing...         154  \n",
       "...                                                 ...         ...  \n",
       "9995  i honestly love the flavour of them i started ...         130  \n",
       "9996  three day on the trot i have been unable to sl...         132  \n",
       "9997  just wanted to say that i have been feeling go...         107  \n",
       "9998  hey guy i just wanted to ask you if you have e...         339  \n",
       "9999  hey all i posted a few day ago about how i wa ...         186  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/depression_bipolar_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-joyce",
   "metadata": {},
   "source": [
    "# \n",
    "### Gridsearch \n",
    "Create a grid of different model paramaters to test out all at once. This should help us refine our model quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-blogger",
   "metadata": {},
   "source": [
    "#### \n",
    "Instantiate the Model Variables. X is our independent variable (which will become vectorized variables) and y is our dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "turned-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['sentences']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-conclusion",
   "metadata": {},
   "source": [
    "# \n",
    "Split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pursuant-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-subcommittee",
   "metadata": {},
   "source": [
    "# \n",
    "Establish a pipeline for our data to go through in the modeling process. First it will go through vectorization with TF-IDF and then the resulting matrix is run through our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-schedule",
   "metadata": {},
   "source": [
    "# \n",
    "This is the grid that our GridSearch will go through. Every different combination will be used to find the optimal model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "technical-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [8_000],\n",
    "    'tvec__max_df': [600, 700, 800],\n",
    "    'tvec__min_df': [20, 25, 30],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__ngram_range': [(1, 2)],\n",
    "    'rf__min_samples_split': [20, 25, 30],\n",
    "    'rf__min_samples_leaf': [1, 2, 3],\n",
    "    'rf__max_features': [90, 100, 110],\n",
    "    'rf__n_estimators': [100],\n",
    "    'rf__max_depth': [90, 100, 110],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-marsh",
   "metadata": {},
   "source": [
    "# \n",
    "Instantiate and fit the GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configured-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=4,\n",
       "             param_grid={'rf__max_depth': [90, 100, 110],\n",
       "                         'rf__max_features': [90, 100, 110],\n",
       "                         'rf__min_samples_leaf': [1, 2, 3],\n",
       "                         'rf__min_samples_split': [20, 25, 30],\n",
       "                         'rf__n_estimators': [100],\n",
       "                         'tvec__max_df': [600, 700, 800],\n",
       "                         'tvec__max_features': [8000],\n",
       "                         'tvec__min_df': [20, 25, 30],\n",
       "                         'tvec__ngram_range': [(1, 2)],\n",
       "                         'tvec__stop_words': ['english']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe_rf, param_grid=pipe_params, cv=3, n_jobs = 4)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-japanese",
   "metadata": {},
   "source": [
    "# \n",
    "Get the optimal scores for the gridsearch as well as the optimal parameters from our grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southern-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "necessary-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "advisory-miller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=700, max_features=8000, min_df=20,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(max_depth=90, max_features=90,\n",
       "                                        min_samples_split=25))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unsigned-whale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 90,\n",
       " 'rf__max_features': 90,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 25,\n",
       " 'rf__n_estimators': 100,\n",
       " 'tvec__max_df': 700,\n",
       " 'tvec__max_features': 8000,\n",
       " 'tvec__min_df': 20,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-advantage",
   "metadata": {},
   "source": [
    "#### \n",
    "These accuracy scores are interesting. It's extremely overfit, but the test accuracy is almost as good as the logistic regression.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-mongolia",
   "metadata": {},
   "source": [
    "# \n",
    "#### Create an individual model with our parameters from the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impossible-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "settled-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec1 = TfidfVectorizer(max_df=700, \n",
    "                       max_features=8000, \n",
    "                       min_df=20,\n",
    "                       ngram_range=(1, 2),\n",
    "                       stop_words='english')\n",
    "\n",
    "rf1 = RandomForestClassifier(max_depth=90, \n",
    "                            max_features=90,\n",
    "                            min_samples_leaf=1,\n",
    "                            min_samples_split=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cooked-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tvec1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "located-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tvec1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "elegant-picnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=90, max_features=90, min_samples_split=25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cubic-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "copyrighted-sweden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-basket",
   "metadata": {},
   "source": [
    "# \n",
    "Get predictions so we can check all of our scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "included-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "separated-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  depression      0.817     0.886     0.850      2000\n",
      "     bipolar      0.876     0.802     0.837      2000\n",
      "\n",
      "    accuracy                          0.844      4000\n",
      "   macro avg      0.846     0.844     0.844      4000\n",
      "weighted avg      0.846     0.844     0.844      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = ['depression', 'bipolar'], digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-drunk",
   "metadata": {},
   "source": [
    "# \n",
    "### Overall Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-physics",
   "metadata": {},
   "source": [
    "The random forest is fascinating. It took significantly longer than logistic regression and dwarfed naive bayes in runtime. But it does have accuracy scores that are almost as high as logistic regression, albeit at the expense of extreme overfitness. Most interestingly though it has the only recall score to be over 80%. Based on purely performance, this is our best model. Let's take a look back at all the models and draw our conclusions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-angel",
   "metadata": {},
   "source": [
    "### \n",
    "[Conclusions](./Conclusions.ipynb)  \n",
    "  \n",
    "[Return to Read Me](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

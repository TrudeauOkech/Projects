{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-explanation",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-mother",
   "metadata": {},
   "source": [
    "# \n",
    "These are our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unlikely-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-beads",
   "metadata": {},
   "source": [
    "# \n",
    "Bring in the cleaned data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stunning-soccer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sentences</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am now homeless and my phone service will tu...</td>\n",
       "      <td>0</td>\n",
       "      <td>['homeless', 'phone', 'service', 'turn', 'real...</td>\n",
       "      <td>i am now homeless and my phone service will tu...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People always describe their depression as con...</td>\n",
       "      <td>0</td>\n",
       "      <td>['people', 'always', 'describe', 'depression',...</td>\n",
       "      <td>people always describe their depression a cons...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been struggling really hard with this. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['struggling', 'really', 'hard', 'graduated', ...</td>\n",
       "      <td>i have been struggling really hard with this w...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So Its been 1 year I had my future secure and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['1', 'year', 'future', 'secure', 'well', 'wa'...</td>\n",
       "      <td>so it been 1 year i had my future secure and w...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am 15 yrs old and I just need some help.\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>['15', 'yr', 'old', 'need', 'help', 'thing', '...</td>\n",
       "      <td>i am 15 yr old and i just need some help thing...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I honestly love the flavour of them!\\n\\nI star...</td>\n",
       "      <td>1</td>\n",
       "      <td>['honestly', 'love', 'flavour', 'started', 're...</td>\n",
       "      <td>i honestly love the flavour of them i started ...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Three days on the trot I have been unable to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['three', 'day', 'trot', 'unable', 'sleep', 'e...</td>\n",
       "      <td>three day on the trot i have been unable to sl...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Just wanted to say that I have been feeling go...</td>\n",
       "      <td>1</td>\n",
       "      <td>['wanted', 'say', 'feeling', 'good', 'last', '...</td>\n",
       "      <td>just wanted to say that i have been feeling go...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Hey guys. I just wanted to ask you if you have...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...</td>\n",
       "      <td>hey guy i just wanted to ask you if you have e...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Hey, all!\\n\\nI posted a few days ago about how...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'posted', 'day', 'ago', 'wa', 'going',...</td>\n",
       "      <td>hey all i posted a few day ago about how i wa ...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               selftext  subreddit  \\\n",
       "0     I am now homeless and my phone service will tu...          0   \n",
       "1     People always describe their depression as con...          0   \n",
       "2     I have been struggling really hard with this. ...          0   \n",
       "3     So Its been 1 year I had my future secure and ...          0   \n",
       "4     I am 15 yrs old and I just need some help.\\n\\n...          0   \n",
       "...                                                 ...        ...   \n",
       "9995  I honestly love the flavour of them!\\n\\nI star...          1   \n",
       "9996  Three days on the trot I have been unable to s...          1   \n",
       "9997  Just wanted to say that I have been feeling go...          1   \n",
       "9998  Hey guys. I just wanted to ask you if you have...          1   \n",
       "9999  Hey, all!\\n\\nI posted a few days ago about how...          1   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     ['homeless', 'phone', 'service', 'turn', 'real...   \n",
       "1     ['people', 'always', 'describe', 'depression',...   \n",
       "2     ['struggling', 'really', 'hard', 'graduated', ...   \n",
       "3     ['1', 'year', 'future', 'secure', 'well', 'wa'...   \n",
       "4     ['15', 'yr', 'old', 'need', 'help', 'thing', '...   \n",
       "...                                                 ...   \n",
       "9995  ['honestly', 'love', 'flavour', 'started', 're...   \n",
       "9996  ['three', 'day', 'trot', 'unable', 'sleep', 'e...   \n",
       "9997  ['wanted', 'say', 'feeling', 'good', 'last', '...   \n",
       "9998  ['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...   \n",
       "9999  ['hey', 'posted', 'day', 'ago', 'wa', 'going',...   \n",
       "\n",
       "                                              sentences  word_count  \n",
       "0     i am now homeless and my phone service will tu...         234  \n",
       "1     people always describe their depression a cons...         155  \n",
       "2     i have been struggling really hard with this w...         394  \n",
       "3     so it been 1 year i had my future secure and w...         105  \n",
       "4     i am 15 yr old and i just need some help thing...         154  \n",
       "...                                                 ...         ...  \n",
       "9995  i honestly love the flavour of them i started ...         130  \n",
       "9996  three day on the trot i have been unable to sl...         132  \n",
       "9997  just wanted to say that i have been feeling go...         107  \n",
       "9998  hey guy i just wanted to ask you if you have e...         339  \n",
       "9999  hey all i posted a few day ago about how i wa ...         186  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/depression_bipolar_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-lloyd",
   "metadata": {},
   "source": [
    "# \n",
    "### Gridsearch \n",
    "Create a grid of different model paramaters to test out all at once. This should help us refine our model quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-debut",
   "metadata": {},
   "source": [
    "#### \n",
    "Instantiate the Model Variables. X is our independent variable (which will become vectorized variables) and y is our dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "duplicate-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['sentences']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-portable",
   "metadata": {},
   "source": [
    "# \n",
    "Split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clean-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-pillow",
   "metadata": {},
   "source": [
    "# \n",
    "Establish a baseline score by getting the percentage of our dominant prediction (in this case it's 50/50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "treated-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-fountain",
   "metadata": {},
   "source": [
    "# \n",
    "Establish a pipeline for our data to go through in the modeling process. First it will go through vectorization with TF-IDF and then the resulting matrix is run through our Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distributed-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-photography",
   "metadata": {},
   "source": [
    "# \n",
    "This is the grid that our GridSearch will go through. Every different combination will be used to find the optimal model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incident-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [6_000, 7_000, 8_000],\n",
    "    'tvec__max_df': [400, 500, 600],\n",
    "    'tvec__min_df': [10, 20, 25],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__ngram_range': [(1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-mouse",
   "metadata": {},
   "source": [
    "# \n",
    "Instantiate and fit the GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "according-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tvec = GridSearchCV(pipe_tvec,\n",
    "                        param_grid = pipe_tvec_params, \n",
    "                        cv=3, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "right-invasion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=4,\n",
       "             param_grid={'tvec__max_df': [400, 500, 600],\n",
       "                         'tvec__max_features': [6000, 7000, 8000],\n",
       "                         'tvec__min_df': [10, 20, 25],\n",
       "                         'tvec__ngram_range': [(1, 2)],\n",
       "                         'tvec__stop_words': ['english']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-skill",
   "metadata": {},
   "source": [
    "# \n",
    "Get the optimal scores for the gridsearch as well as the optimal parameters from our grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beneficial-victoria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8753333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focused-western",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thorough-sierra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=500, max_features=6000, min_df=10,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hydraulic-rouge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 500,\n",
       " 'tvec__max_features': 6000,\n",
       " 'tvec__min_df': 10,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-spanish",
   "metadata": {},
   "source": [
    "#### \n",
    "These accuracy scores are good. It's a little overfit, but not terribly. Considering all of the different models we ran, this is probably the most realistic variance\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-input",
   "metadata": {},
   "source": [
    "# \n",
    "#### Create an individual model with our parameters from the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intelligent-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opponent-reservoir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cosmetic-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec1 = TfidfVectorizer(max_df=500, \n",
    "                        max_features=6000, \n",
    "                        min_df=10,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words=['english']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polished-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tvec1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cathedral-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tvec1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bronze-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dangerous-battle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pharmaceutical-retirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continental-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-string",
   "metadata": {},
   "source": [
    "# \n",
    "Get predictions so we can check all of our scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polished-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "public-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  depression      0.800     0.858     0.828      2000\n",
      "     bipolar      0.847     0.785     0.815      2000\n",
      "\n",
      "    accuracy                          0.822      4000\n",
      "   macro avg      0.823     0.822     0.821      4000\n",
      "weighted avg      0.823     0.822     0.821      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = ['depression', 'bipolar'], digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-ancient",
   "metadata": {},
   "source": [
    "# \n",
    "### Overall Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-snake",
   "metadata": {},
   "source": [
    "Naive Bayes did much better than the baseline model. We're off to a really good start with our modeling. The accuracy is over 80% and the recall(sensitivity) is just below 80%. Let's see if logistic regression performs any better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-consent",
   "metadata": {},
   "source": [
    "# \n",
    "**Up Next:**  \n",
    "[Logistic Regression Model](./Logistic_Regression_Model.ipynb)  \n",
    "  \n",
    "[Return to Read Me](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

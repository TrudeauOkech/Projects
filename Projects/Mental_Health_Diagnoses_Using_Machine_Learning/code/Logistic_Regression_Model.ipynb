{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cheap-nevada",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-september",
   "metadata": {},
   "source": [
    "# \n",
    "These are our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-depression",
   "metadata": {},
   "source": [
    "# \n",
    "Bring in the cleaned data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heated-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>sentences</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am now homeless and my phone service will tu...</td>\n",
       "      <td>0</td>\n",
       "      <td>['homeless', 'phone', 'service', 'turn', 'real...</td>\n",
       "      <td>i am now homeless and my phone service will tu...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People always describe their depression as con...</td>\n",
       "      <td>0</td>\n",
       "      <td>['people', 'always', 'describe', 'depression',...</td>\n",
       "      <td>people always describe their depression a cons...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been struggling really hard with this. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['struggling', 'really', 'hard', 'graduated', ...</td>\n",
       "      <td>i have been struggling really hard with this w...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So Its been 1 year I had my future secure and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['1', 'year', 'future', 'secure', 'well', 'wa'...</td>\n",
       "      <td>so it been 1 year i had my future secure and w...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am 15 yrs old and I just need some help.\\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>['15', 'yr', 'old', 'need', 'help', 'thing', '...</td>\n",
       "      <td>i am 15 yr old and i just need some help thing...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I honestly love the flavour of them!\\n\\nI star...</td>\n",
       "      <td>1</td>\n",
       "      <td>['honestly', 'love', 'flavour', 'started', 're...</td>\n",
       "      <td>i honestly love the flavour of them i started ...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Three days on the trot I have been unable to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['three', 'day', 'trot', 'unable', 'sleep', 'e...</td>\n",
       "      <td>three day on the trot i have been unable to sl...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Just wanted to say that I have been feeling go...</td>\n",
       "      <td>1</td>\n",
       "      <td>['wanted', 'say', 'feeling', 'good', 'last', '...</td>\n",
       "      <td>just wanted to say that i have been feeling go...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Hey guys. I just wanted to ask you if you have...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...</td>\n",
       "      <td>hey guy i just wanted to ask you if you have e...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Hey, all!\\n\\nI posted a few days ago about how...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hey', 'posted', 'day', 'ago', 'wa', 'going',...</td>\n",
       "      <td>hey all i posted a few day ago about how i wa ...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               selftext  subreddit  \\\n",
       "0     I am now homeless and my phone service will tu...          0   \n",
       "1     People always describe their depression as con...          0   \n",
       "2     I have been struggling really hard with this. ...          0   \n",
       "3     So Its been 1 year I had my future secure and ...          0   \n",
       "4     I am 15 yrs old and I just need some help.\\n\\n...          0   \n",
       "...                                                 ...        ...   \n",
       "9995  I honestly love the flavour of them!\\n\\nI star...          1   \n",
       "9996  Three days on the trot I have been unable to s...          1   \n",
       "9997  Just wanted to say that I have been feeling go...          1   \n",
       "9998  Hey guys. I just wanted to ask you if you have...          1   \n",
       "9999  Hey, all!\\n\\nI posted a few days ago about how...          1   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     ['homeless', 'phone', 'service', 'turn', 'real...   \n",
       "1     ['people', 'always', 'describe', 'depression',...   \n",
       "2     ['struggling', 'really', 'hard', 'graduated', ...   \n",
       "3     ['1', 'year', 'future', 'secure', 'well', 'wa'...   \n",
       "4     ['15', 'yr', 'old', 'need', 'help', 'thing', '...   \n",
       "...                                                 ...   \n",
       "9995  ['honestly', 'love', 'flavour', 'started', 're...   \n",
       "9996  ['three', 'day', 'trot', 'unable', 'sleep', 'e...   \n",
       "9997  ['wanted', 'say', 'feeling', 'good', 'last', '...   \n",
       "9998  ['hey', 'guy', 'wanted', 'ask', 'ever', 'probl...   \n",
       "9999  ['hey', 'posted', 'day', 'ago', 'wa', 'going',...   \n",
       "\n",
       "                                              sentences  word_count  \n",
       "0     i am now homeless and my phone service will tu...         234  \n",
       "1     people always describe their depression a cons...         155  \n",
       "2     i have been struggling really hard with this w...         394  \n",
       "3     so it been 1 year i had my future secure and w...         105  \n",
       "4     i am 15 yr old and i just need some help thing...         154  \n",
       "...                                                 ...         ...  \n",
       "9995  i honestly love the flavour of them i started ...         130  \n",
       "9996  three day on the trot i have been unable to sl...         132  \n",
       "9997  just wanted to say that i have been feeling go...         107  \n",
       "9998  hey guy i just wanted to ask you if you have e...         339  \n",
       "9999  hey all i posted a few day ago about how i wa ...         186  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/depression_bipolar_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-weight",
   "metadata": {},
   "source": [
    "# \n",
    "### Gridsearch \n",
    "Create a grid of different model paramaters to test out all at once. This should help us refine our model quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-landscape",
   "metadata": {},
   "source": [
    "#### \n",
    "Instantiate the Model Variables. X is our independent variable (which will become vectorized variables) and y is our dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescription-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['sentences']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-familiar",
   "metadata": {},
   "source": [
    "# \n",
    "Split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "social-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-conflict",
   "metadata": {},
   "source": [
    "# \n",
    "Establish a baseline score by getting the percentage of our dominant prediction (in this case it's 50/50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personalized-zoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-television",
   "metadata": {},
   "source": [
    "# \n",
    "Establish a pipeline for our data to go through in the modeling process. First it will go through vectorization with TF-IDF and then the resulting matrix is run through our logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eastern-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-startup",
   "metadata": {},
   "source": [
    "# \n",
    "This is the grid that our GridSearch will go through. Every different combination will be used to find the optimal model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quantitative-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [7_000, 8_000, 9_000],\n",
    "    'tvec__max_df': [600, 700, 800],\n",
    "    'tvec__min_df': [20, 25, 30],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'logreg__C': [.1, 1, 10], \n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-wisconsin",
   "metadata": {},
   "source": [
    "# \n",
    "Instantiate and fit the GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hybrid-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg = GridSearchCV(pipe_tvec,\n",
    "                        param_grid = pipe_tvec_params, \n",
    "                        cv=5, n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "threaded-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('logreg', LogisticRegression())]),\n",
       "             n_jobs=2,\n",
       "             param_grid={'logreg__C': [0.1, 1, 10],\n",
       "                         'logreg__penalty': ['l1', 'l2'],\n",
       "                         'logreg__solver': ['liblinear'],\n",
       "                         'tvec__max_df': [600, 700, 800],\n",
       "                         'tvec__max_features': [7000, 8000, 9000],\n",
       "                         'tvec__min_df': [20, 25, 30],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': ['english']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-validity",
   "metadata": {},
   "source": [
    "# \n",
    "Get the optimal scores for the gridsearch as well as the optimal parameters from our grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intended-appearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polyphonic-interpretation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decreased-illustration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=800, max_features=7000, min_df=30,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1, penalty='l1', solver='liblinear'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bizarre-implementation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1,\n",
       " 'logreg__penalty': 'l1',\n",
       " 'logreg__solver': 'liblinear',\n",
       " 'tvec__max_df': 800,\n",
       " 'tvec__max_features': 7000,\n",
       " 'tvec__min_df': 30,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-killing",
   "metadata": {},
   "source": [
    "#### \n",
    "These accuracy scores are really good. It's also not overfit. The variance is pretty low.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-broadcasting",
   "metadata": {},
   "source": [
    "# \n",
    "#### Create an individual model with our parameters from the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exact-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "referenced-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec1 = TfidfVectorizer(max_df = 800,\n",
    "                        max_features = 7000, \n",
    "                        min_df = 30, \n",
    "                        ngram_range = (1, 2), \n",
    "                        stop_words = 'english')\n",
    "\n",
    "logreg1 = LogisticRegression(C = 1, \n",
    "                             penalty = 'l1', \n",
    "                             solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "independent-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tvec1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "underlying-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tvec1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acoustic-chocolate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intensive-fields",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coastal-lightweight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-improvement",
   "metadata": {},
   "source": [
    "# \n",
    "Get predictions so we can check all of our scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legal-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indian-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  depression      0.806     0.915     0.857      2000\n",
      "     bipolar      0.902     0.779     0.836      2000\n",
      "\n",
      "    accuracy                          0.848      4000\n",
      "   macro avg      0.854     0.847     0.847      4000\n",
      "weighted avg      0.854     0.848     0.847      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = ['depression', 'bipolar'], digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-massage",
   "metadata": {},
   "source": [
    "# \n",
    "### Overall Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-national",
   "metadata": {},
   "source": [
    "The logistic regression has our best accuracy scores, but when we take a look at the recall score(sensitivity) of 0.779, it's clear that this model is lacking. This means that there's more people with bipolar disorder who aren't going to get diagnosed as such. That could have bad consequences even if we have a higher overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-shopping",
   "metadata": {},
   "source": [
    "# \n",
    "**Up Next:**  \n",
    "[Random Forest Model](./Random_Forest_Model.ipynb)  \n",
    "  \n",
    "[Return to Read Me](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
